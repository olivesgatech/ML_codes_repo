{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 0.000\n",
      "MSE on Test Data: 99036.313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# house_prices = load_boston()\n",
    "# X, y = house_prices.data, house_prices.target\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "n_train = 10\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# normalize X\n",
    "#X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True) \n",
    "\n",
    "# low rank approximation of X\n",
    "# rank = np.linalg.matrix_rank(X)\n",
    "# u, s, vh = np.linalg.svd(X)\n",
    "# X = u[:,:rank].dot(np.diag(s)[:rank,:rank]).dot(vh[:rank,:])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "lambda_ = 0\n",
    "theta = np.linalg.inv(X_train.T.dot(X_train)+lambda_*np.eye(X_train.shape[1])).dot(X_train.T.dot(y_train))\n",
    "\n",
    "# test\n",
    "y_pred = X_test.dot(theta)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((X_train.dot(theta) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_test.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Implementing Ordinary Least Squares Regression\n",
    "\n",
    "You are provided a class template called `MyLeastSquares` to implement your very own least squares class below. You are required to fill in the `fit` and `predict()` methods of the class. Make sure that the functions are able to take in inputs and return outputs of the shape specified in the function definitions. Run the cell after you complete this to train and predict your regressor on the diabetes dataset provided by the sklearn library. If everything goes right, you should see training and test errors in the range $1000$ to $5000$. Use the normal equation method of solving regression problems as described in the lecture slides. *Do not forget to account for the intercept term!* (**_Hint:_** It may be helpful to look into `numpy.linalg.inv` and `numpy.matmul` functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 3203.340\n",
      "MSE on Test Data: 3282.631\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyLeastSquares:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression\"\"\"\n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        self.theta = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(self.y_train))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 40\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Singular Value Decomposition and Least Squares\n",
    "\n",
    "A problem that can sometimes happen with regression methods is collinearity of features (i.e., some features are linear combinations of other features.) This results in an ill-conditioned data matrix $X$ which can in turn sometimes lead to errors while computing the inverses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-882-cff633942922>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# train theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mLS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyLeastSquares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mLS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-7d3fe8296c9d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# append ones for bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rank_truncate(X, rank):\n",
    "    \"\"\"Function perfroms rank truncation for given feature matrix X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array_like, shape(N,D).\n",
    "        ndarray containing N training examples with D features each.\n",
    "    \n",
    "    rank: int\n",
    "        integer specifying the number of singular values to truncate with.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_trunc: array_like, shape(N,D)\n",
    "        ndarray containing rank-approximation of X.\n",
    "    \"\"\"\n",
    "    \n",
    "    u, s, vh = np.linalg.svd(X)\n",
    "    X_trunc = u[:,:rank].dot(np.diag(s)[:rank,:rank]).dot(vh[:rank,:])\n",
    "    \n",
    "    return X_trunc\n",
    "    \n",
    "    \n",
    "# load dataset\n",
    "house_prices = load_boston()\n",
    "X, y = house_prices.data, house_prices.target\n",
    "X[:,[-1]] = np.sum(X[:,:-1],axis=1,keepdims=True) \n",
    "\n",
    "# train-test split\n",
    "n_train = 30\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation  \n",
    "# rank = np.linalg.matrix_rank(X)  # uncomment for svd implementation\n",
    "# X = rank_truncate(X, rank)  # uncomment for svd implementation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Singular Value Decomposition and Least Squares\n",
    "\n",
    "A problem that can sometimes happen with regression methods is collinearity of features (i.e., some features are linear combinations of other features.) This results in an ill-conditioned data matrix $X$ which can in turn lead to unstable solutions or even singular matrices (with no inverse). The underlying cause of this instability is the very small (or even zero-valued) singular values in the singular value decomposition (SVD) of the data matrix $X$, stemming from the fact that one or more of its columns happen to be linear combinations of the rest. A potential solution around this problem is to then reconstruct $X$ via a low rank matrix approximation,$\\hat{X}$, using only the largest singular values in the original SVD of X. This leads to a more stable solution for the regression algorithm than if used with the original $X$. \n",
    "\n",
    "For an $N\\times D$ matrix $X$ and a corresponding $N\\times 1$, the least squares formulation may be stated as:\n",
    "\n",
    "\\begin{equation}\n",
    "X\\mathbf{\\theta} = \\mathbf{y}, \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta$ is a $D\\times 1$ parameter vector. The SVD for $X$ may then be substituted as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "U\\Sigma V^{T}\\mathbf{\\theta} = \\mathbf{y}, \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $U$ is an $N\\times N$ orthogonal matrix (containing the left singular vectors), $\\Sigma$ is an $N\\times D$ matrix (containing the singular values on its main diagonal and zeros elsewhere), and $V$ is an $D\\times D$ orthogonal matrix (containing the right singular vectors). An $r$-rank approximation of $X$ is given as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{X}_{r} = U(:,:r)\\times\\Sigma(:r,:r)\\times V(:r,:) \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Replacing $X$ by $\\hat{X}_{r}$ in Equation 2 followed by the inverse, we obtain the following solution for theta:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{U}_{r}\\hat{\\Sigma}_{r}\\hat{V}_{r}^{T}\\mathbf{\\theta} &= \\mathbf{y}, \\tag{4} \\\\\n",
    "\\mathbf{\\hat{\\theta}} &= \\hat{V}_{r}\\hat{\\Sigma}_{r}^{-1}\\hat{U}_{r}^{T}\\mathbf{y}. \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "You are provided below the Boston House prices Dataset each training examples for which originally contained 13 features. We changed the data to replace one of those features to be a simple linear combination of the others, resulting in a rank-deficient data matrix X. For this question, perform the following:\n",
    "\n",
    "**(a)** For the class `MySVDLeastSquares` given below, implement the `fit()` and `predict()` methods as before. In particular, the `fit()` method computes the $\\theta$ vector via a low rank approximation of $X$ as depicted in Equation 5 above, with the rank provided by the user. *Don't forget to account for the intercept term in the $\\theta$ vector!* \n",
    "\n",
    "**(b)** Run the cell underneath to perform a stability analysis of SVD-based least squares method to target values containing random noise. What do you observe? Describe and explain with reference to the underlying phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 3311.049\n",
      "MSE on Test Data: 91181.267\n"
     ]
    }
   ],
   "source": [
    "# for part (a)\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "class MySVDLeastSquares:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def fit(self, rank):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression via a low rank approximation\n",
    "        with given rank\"\"\"\n",
    "        \n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        u, s, vh = np.linalg.svd(X) \n",
    "        rank = rank\n",
    "        self.theta = vh.T[:,:rank].dot(np.diag(1/s)[:rank,:rank]).dot(u.T[:rank,:]).dot(self.y_train.reshape(-1,1))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred    \n",
    "    \n",
    "# load dataset\n",
    "house_prices = load_boston()\n",
    "X, y = house_prices.data, house_prices.target\n",
    "X[:,[-1]] = np.sum(X[:,:-1],axis=1,keepdims=True) \n",
    "\n",
    "# train-test split\n",
    "n_train = 30\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MySVDLeastSquares(X_train, y_train)\n",
    "LS.fit(rank=14)\n",
    "\n",
    "# # test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((X_train.dot(theta) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for part (b)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset\n",
    "house_prices = load_boston()\n",
    "X, y = house_prices.data, house_prices.target\n",
    "X[:,[-1]] = np.sum(X[:,:-1],axis=1,keepdims=True) \n",
    "\n",
    "# train-test split\n",
    "n_train = 30\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,7))\n",
    "\n",
    "num_trials = 50  # fix number of trials for each experiment\n",
    "\n",
    "# full rank approximation of X\n",
    "mse_train = []  # array to collect mses for training set\n",
    "mse_test = []  # array to collect mses for training set\n",
    "for i in range(num_trials):\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "    \n",
    "    # add noise to y_train\n",
    "    y_train += 0.1*np.random.normal(y_train.mean(), 0.5*y_train.std(), size=y_train.shape)\n",
    "    \n",
    "    # train theta\n",
    "    LS = MySVDLeastSquares(X_train, y_train)\n",
    "    LS.fit(rank=14)\n",
    "\n",
    "    # test\n",
    "    y_pred = LS.predict(X_test)\n",
    "    \n",
    "    mse_train.append(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size)  # store mse train\n",
    "    mse_test.append(np.sum((LS.predict(X_test) - y_train)**2)/y_train.size)  # store mse test\n",
    "   \n",
    "ax1.hist(mse_train, bins = 50, color='r', label='Training MSE')\n",
    "ax1.hist(mse_test, bins = 50, color='b', label='Test MSE')\n",
    "ax1.set_title('MSE Distribution for Full-Rank SVD-Based Least Squares')\n",
    "ax1.set_xlabel('MSE Values')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "# low rank approximation of X\n",
    "mse_train = []  # array to collect mses for training set\n",
    "mse_test = []  # array to collect mses for training set\n",
    "for i in range(num_trials):\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "    \n",
    "    # add noise to y_train\n",
    "    y_train += 0.1*np.random.normal(y_train.mean(), 0.5*y_train.std(), size=y_train.shape)\n",
    "    \n",
    "    # train theta\n",
    "    LS = MySVDLeastSquares(X_train, y_train)\n",
    "    LS.fit(rank=7)\n",
    "\n",
    "    # test\n",
    "    y_pred = LS.predict(X_test)\n",
    "    \n",
    "    mse_train.append(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size)  # store mse train\n",
    "    mse_test.append(np.sum((LS.predict(X_test) - y_train)**2)/y_train.size)  # store mse test\n",
    "\n",
    "    \n",
    "ax2.hist(mse_train, bins = 50, color='r', label='Training MSE')\n",
    "ax2.hist(mse_test, bins = 50, color='b', label='Test MSE')\n",
    "ax2.set_title('MSE Distribution for Low-Rank SVD-Based Least Squares')\n",
    "ax2.set_xlabel('MSE Values')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Solution to (b)_**\n",
    "\n",
    "It is observed that the MSEs for both test and train sets show a much greater scatter with full rank SVD-based least squares method compared to the low rank version. It shows that the latter is more robust to noises in target data corresponding to ill-conditioned data matrices with collinear features. The reason may be states as follows:\n",
    "\n",
    "The full rank version of Equation 5 may be rewritten as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\hat{\\theta}} &= \\sum^{D}_{i=1} \\frac{\\mathbf{v}_{i}\\mathbf{u}_{i}^{T}\\mathbf{y}}{\\sigma_{i}}\n",
    "\\end{align}\n",
    "\n",
    "Small values of \\$sigma$ in the denominaor introduce large  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3415.850328795372"
      ]
     },
     "execution_count": 1563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1)"
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS.theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 14)"
      ]
     },
     "execution_count": 1298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 536.476\n",
      "MSE on Test Data: 1495.532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rank_truncate(X, rank):\n",
    "    \"\"\"Function perfroms rank truncation for given feature matrix X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array_like, shape(N,D).\n",
    "        ndarray containing N training examples with D features each.\n",
    "    \n",
    "    rank: int\n",
    "        integer specifying the number of singular values to truncate with.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_trunc: array_like, shape(N,D)\n",
    "        ndarray containing rank-approximation of X.\n",
    "    \"\"\"\n",
    "    \n",
    "    u, s, vh = np.linalg.svd(X)\n",
    "    X_trunc = u[:,:rank].dot(np.diag(s)[:rank,:rank]).dot(vh[:rank,:])\n",
    "    \n",
    "    return X_trunc\n",
    "    \n",
    "    \n",
    "# load dataset\n",
    "house_prices = load_boston()\n",
    "X, y = house_prices.data, house_prices.target\n",
    "X[:,[-1]] = np.sum(X[:,:-1],axis=1,keepdims=True) \n",
    "\n",
    "# train-test split\n",
    "n_train = 30\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation  \n",
    "#rank = np.linalg.matrix_rank(X)  # uncomment for svd implementation\n",
    "#X = rank_truncate(X, rank)  # uncomment for svd implementation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 870.333\n",
      "MSE on Test Data: 2417.588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_linnerud\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def gen_polynomial_features(X, power):\n",
    "    \"\"\"Function generates polynomial features of specified power and appends to X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array_like, shape(N,D).\n",
    "        ndarray of feature matrix containing N training examples with D features each.\n",
    "        \n",
    "    power: int\n",
    "        integer specifying power X is raised to before appended to itself.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly: array_like, shape(N,2D)\n",
    "        ndarray of appended polynomial features to original feature matrix\"\"\"\n",
    "    \n",
    "    X_poly = np.concatenate((X, X**power), axis=1)\n",
    "    \n",
    "    return X_poly\n",
    "    \n",
    "# load dataset\n",
    "exercises = load_linnerud()\n",
    "X, y = exercises.data, exercises.target[:,0]\n",
    "\n",
    "# train-test split\n",
    "n_train = 3\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "#generate polynomial features\n",
    "#X = gen_polynomial_features(X, power=2)\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 9599.535\n",
      "MSE on Test Data: 5594.086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyRidgeLeastSquares:\n",
    "    def __init__(self, X_train, y_train, alpha=0):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\n",
    "        \n",
    "        alpha: float\n",
    "            float specifying the multiplier of the L2 penalty in Ridge loss\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression\"\"\"\n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        self.theta = np.linalg.inv(X.T.dot(X) + self.alpha*np.eye(X.shape[1])).dot(X.T.dot(self.y_train))\n",
    "     \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 10\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyRidgeLeastSquares(X_train, y_train, alpha=2)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 54.653 | theta norm: 1.119\n",
      "Cost: 50.950 | theta norm: 0.801\n",
      "Cost: 50.654 | theta norm: 0.804\n",
      "Cost: 50.919 | theta norm: 0.838\n",
      "Cost: 50.874 | theta norm: 0.986\n",
      "Cost: 50.996 | theta norm: 0.960\n",
      "Cost: 50.839 | theta norm: 0.888\n",
      "Cost: 50.676 | theta norm: 0.951\n",
      "Cost: 50.790 | theta norm: 0.789\n",
      "Cost: 50.922 | theta norm: 0.828\n",
      "Cost: 50.881 | theta norm: 0.865\n",
      "Cost: 50.865 | theta norm: 0.864\n",
      "Cost: 50.896 | theta norm: 0.864\n",
      "Cost: 50.848 | theta norm: 0.784\n",
      "Cost: 50.767 | theta norm: 0.819\n",
      "Cost: 50.611 | theta norm: 0.983\n",
      "Cost: 50.757 | theta norm: 0.829\n",
      "Cost: 50.750 | theta norm: 0.888\n",
      "Cost: 50.781 | theta norm: 0.857\n",
      "Cost: 50.941 | theta norm: 0.956\n",
      "Cost: 50.945 | theta norm: 0.979\n",
      "Cost: 50.783 | theta norm: 0.834\n",
      "Cost: 50.954 | theta norm: 0.881\n",
      "Cost: 50.761 | theta norm: 0.895\n",
      "Cost: 50.697 | theta norm: 0.932\n",
      "Cost: 50.743 | theta norm: 0.960\n",
      "Cost: 50.762 | theta norm: 0.969\n",
      "Cost: 50.878 | theta norm: 0.847\n",
      "Cost: 51.174 | theta norm: 0.966\n",
      "Cost: 50.656 | theta norm: 0.869\n",
      "Cost: 50.999 | theta norm: 0.923\n",
      "Cost: 50.634 | theta norm: 0.988\n",
      "Cost: 50.957 | theta norm: 0.993\n",
      "Cost: 50.833 | theta norm: 0.843\n",
      "Cost: 50.860 | theta norm: 0.868\n",
      "Cost: 50.662 | theta norm: 0.847\n",
      "Cost: 50.931 | theta norm: 0.928\n",
      "Cost: 51.022 | theta norm: 0.956\n",
      "Cost: 50.889 | theta norm: 0.813\n",
      "Cost: 50.685 | theta norm: 0.946\n",
      "MSE on Training Data: 50.593\n",
      "MSE on Test Data: 950.126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyLassoLeastSquares:\n",
    "    def __init__(self, X_train, y_train, beta=0):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\n",
    "        \n",
    "        beta: float\n",
    "            float specifying the multiplier of the L1 penalty in Ridge loss\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.beta = beta\n",
    "        self.theta = np.random.randn(X_train.shape[1]+1, 1)\n",
    "        \n",
    "    def gradient(self, X):\n",
    "        \"\"\"Function computes gradient of Lasso cost.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array_like, shape(N, D+1)\n",
    "            feature matrix containing N training examples containing D+1 features each\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad = array_like, shape(D+1, 1).\n",
    "            gradient vector containing gradient for each element in theta vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        lasso_grad = np.ones(self.theta.shape)\n",
    "        lasso_grad[self.theta<=0] = -1\n",
    "        grad = (2*X.T.dot(X.dot(self.theta) - self.y_train.reshape(-1,1)))/X.shape[0] + self.beta*lasso_grad\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    \n",
    "    def fit(self, num_epochs, step):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_epochs: int,\n",
    "            integer specifying number of trianing epochs\n",
    "            \n",
    "        step: float\n",
    "            float specifying step size for gradient descent\n",
    "        \"\"\"\n",
    "                \n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        \n",
    "        # warm start\n",
    "        self.theta = np.linalg.inv(X.T.dot(X) + 0.0*np.eye(X.shape[1])).dot(X.T.dot(self.y_train)).reshape(-1,1)\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            cost = np.sum((y_train-X.dot(self.theta))**2)/X.shape[0] + self.beta*np.sum(np.abs(self.theta))\n",
    "#             self.theta = self.theta - step*self.gradient(X) # simple gradient descent\n",
    "            \n",
    "            \n",
    "#             for j in range(self.theta.size):\n",
    "#                 self.theta[j] = self.theta[j] - step*self.gradient(X)[j]                                  # coordinate descent\n",
    "                \n",
    "            self.theta = self.theta - step*np.linalg.inv(2/X.shape[0] * X.T.dot(X)).dot(self.gradient(X))  # newton's method\n",
    "            \n",
    "            if i%50==0:\n",
    "                print('Cost: {:0.3f} | theta norm: {:0.3f}'.format(cost, np.linalg.norm(self.theta)))\n",
    "\n",
    "                \n",
    "                \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_boston()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 20\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "# normalize X\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True) \n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "#train theta\n",
    "LS = MyLassoLeastSquares(X_train, y_train, beta=0.1)\n",
    "LS.fit(num_epochs=2000, step=0.1)\n",
    "\n",
    "# LS = MyLeastSquares(X_train, y_train)\n",
    "# LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
