{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 0.000\n",
      "MSE on Test Data: 99036.313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# house_prices = load_boston()\n",
    "# X, y = house_prices.data, house_prices.target\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "n_train = 10\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# normalize X\n",
    "#X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True) \n",
    "\n",
    "# low rank approximation of X\n",
    "# rank = np.linalg.matrix_rank(X)\n",
    "# u, s, vh = np.linalg.svd(X)\n",
    "# X = u[:,:rank].dot(np.diag(s)[:rank,:rank]).dot(vh[:rank,:])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "lambda_ = 0\n",
    "theta = np.linalg.inv(X_train.T.dot(X_train)+lambda_*np.eye(X_train.shape[1])).dot(X_train.T.dot(y_train))\n",
    "\n",
    "# test\n",
    "y_pred = X_test.dot(theta)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((X_train.dot(theta) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_test.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 3203.340\n",
      "MSE on Test Data: 3282.631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyLeastSquares:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression\"\"\"\n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        self.theta = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(self.y_train))\n",
    "     \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 40\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 2466.863\n",
      "MSE on Test Data: 67647433114498321506257296424960.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rank_truncate(X, rank):\n",
    "    \"\"\"Function perfroms rank truncation for given feature matrix X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array_like, shape(N,D).\n",
    "        ndarray containing N training examples with D features each.\n",
    "    \n",
    "    rank: int\n",
    "        integer specifying the number of singular values to truncate with.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_trunc: array_like, shape(N,D)\n",
    "        ndarray containing rank-approximation of X.\n",
    "    \"\"\"\n",
    "    \n",
    "    u, s, vh = np.linalg.svd(X)\n",
    "    X_trunc = u[:,:rank].dot(np.diag(s)[:rank,:rank]).dot(vh[:rank,:])\n",
    "    \n",
    "    return X_trunc\n",
    "    \n",
    "    \n",
    "# load dataset\n",
    "house_prices = load_boston()\n",
    "X, y = house_prices.data, house_prices.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 10\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 870.333\n",
      "MSE on Test Data: 2417.588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_linnerud\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def gen_polynomial_features(X, power):\n",
    "    \"\"\"Function generates polynomial features of specified power and appends to X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array_like, shape(N,D).\n",
    "        ndarray of feature matrix containing N training examples with D features each.\n",
    "        \n",
    "    power: int\n",
    "        integer specifying power X is raised to before appended to itself.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly: array_like, shape(N,2D)\n",
    "        ndarray of appended polynomial features to original feature matrix\"\"\"\n",
    "    \n",
    "    X_poly = np.concatenate((X, X**power), axis=1)\n",
    "    \n",
    "    return X_poly\n",
    "    \n",
    "# load dataset\n",
    "exercises = load_linnerud()\n",
    "X, y = exercises.data, exercises.target[:,0]\n",
    "\n",
    "# train-test split\n",
    "n_train = 3\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "#generate polynomial features\n",
    "#X = gen_polynomial_features(X, power=2)\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLeastSquares(X_train, y_train)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Data: 9599.535\n",
      "MSE on Test Data: 5594.086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyRidgeLeastSquares:\n",
    "    def __init__(self, X_train, y_train, alpha=0):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\n",
    "        \n",
    "        alpha: float\n",
    "            float specifying the multiplier of the L2 penalty in Ridge loss\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression\"\"\"\n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        self.theta = np.linalg.inv(X.T.dot(X) + self.alpha*np.eye(X.shape[1])).dot(X.T.dot(self.y_train))\n",
    "     \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 10\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyRidgeLeastSquares(X_train, y_train, alpha=2)\n",
    "LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 351.918 | theta norm: 2.896\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "Cost: 56.865 | theta norm: 5.171\n",
      "Cost: 56.764 | theta norm: 5.187\n",
      "MSE on Training Data: 55.868\n",
      "MSE on Test Data: 2206.932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyLassoLeastSquares:\n",
    "    def __init__(self, X_train, y_train, beta=0):\n",
    "        \"\"\"Function stores feature matrix and corresponding target data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train: array_like, shape(N,D)\n",
    "            ndarray containing N training examples, each with D feature values.\n",
    "            \n",
    "        y_train: array_like, shape(N,1)\n",
    "            ndarray containing target values for each of N examples in X_train.\n",
    "        \n",
    "        beta: float\n",
    "            float specifying the multiplier of the L1 penalty in Ridge loss\"\"\"\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.beta = beta\n",
    "        self.theta = np.random.randn(X_train.shape[1]+1, 1)\n",
    "        \n",
    "    def gradient(self, X):\n",
    "        \"\"\"Function computes gradient of Lasso cost.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array_like, shape(N, D+1)\n",
    "            feature matrix containing N training examples containing D+1 features each\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad = array_like, shape(D+1, 1).\n",
    "            gradient vector containing gradient for each element in theta vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        lasso_grad = np.ones(self.theta.shape)\n",
    "        lasso_grad[self.theta<=0] = -1\n",
    "        grad = (2*X.T.dot(X.dot(self.theta) - self.y_train.reshape(-1,1)))/X.shape[0] + self.beta*lasso_grad\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    \n",
    "    def fit(self, num_epochs, step):\n",
    "        \"\"\"Function computes the weight vector of shape (D+1, 1) for regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_epochs: int,\n",
    "            integer specifying number of trianing epochs\n",
    "            \n",
    "        step: float\n",
    "            float specifying step size for gradient descent\n",
    "        \"\"\"\n",
    "                \n",
    "        # append ones for bias\n",
    "        X = np.concatenate((self.X_train, np.ones((self.X_train.shape[0],1))), axis=1)\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            cost = np.sum((y_train-X.dot(self.theta))**2)/X.shape[0] + self.beta*np.sum(np.abs(self.theta))\n",
    "#             self.theta = self.theta - step*self.gradient(X) # simple gradient descent\n",
    "            \n",
    "            \n",
    "#             for j in range(self.theta.size):\n",
    "#                 self.theta[j] = self.theta[j] - step*self.gradient(X)[j]                                  # coordinate descent\n",
    "                \n",
    "            self.theta = self.theta - step*np.linalg.inv(2/X.shape[0] * X.T.dot(X)).dot(self.gradient(X))  # newton's method\n",
    "            \n",
    "            if i%50==0:\n",
    "                print('Cost: {:0.3f} | theta norm: {:0.3f}'.format(cost, np.linalg.norm(self.theta)))\n",
    "\n",
    "                \n",
    "                \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Function predicts targets for given X_test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_test: array_like, shape(N,D)\n",
    "            ndarray containing N test examples with D features each.\n",
    "            \n",
    "        Returns: array_like, shape(N,1).\n",
    "            ndarray containing predicted targets of shape (N,1).\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.concatenate((X_test, np.ones((X_test.shape[0],1))), axis=1)\n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# load dataset\n",
    "diabetes = load_boston()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# train-test split\n",
    "n_train = 20\n",
    "n_test = X.shape[0] - n_train\n",
    "\n",
    "# perform rank truncation\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "X = rank_truncate(X, rank)\n",
    "\n",
    "# normalize X\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True) \n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test, random_state=4803)\n",
    "\n",
    "# train theta\n",
    "LS = MyLassoLeastSquares(X_train, y_train, beta=0.1)\n",
    "LS.fit(num_epochs=2000, step=0.5)\n",
    "\n",
    "# LS = MyLeastSquares(X_train, y_train)\n",
    "# LS.fit()\n",
    "\n",
    "# test\n",
    "y_pred = LS.predict(X_test)\n",
    "\n",
    "# evaluate performance\n",
    "print('MSE on Training Data: {:0.3f}'.format(np.sum((LS.predict(X_train) - y_train)**2)/y_train.size))\n",
    "print('MSE on Test Data: {:0.3f}'.format(np.sum((y_pred - y_test)**2)/y_pred.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.73011186]\n",
      " [ 1.5612985 ]\n",
      " [-0.48763894]] \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "temp = np.random.randn(3,1)\n",
    "temp2 = np.ones(temp.shape)\n",
    "print(temp, '\\n',temp2)\n",
    "temp2[temp<=0] = -1\n",
    "print(temp2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
